# =============================================================================
# Ollama推論設定ファイル
# =============================================================================
# 使用方法: 
#   python3 scripts/inference.py --config configs/inference_example.yaml
#   python3 scripts/inference.py --config configs/inference_example.yaml --prompt "こんにちは"
# =============================================================================

# 説明（任意）
description: "Ollama inference configuration example"

# =============================================================================
# モデル設定
# =============================================================================
model:
  # Ollamaモデル名
  # ベースモデルの例:
  #   - llama2
  #   - llama3
  #   - mistral
  #   - gemma
  #   - qwen2
  # 学習済みモデルの場合は、Ollamaに登録したモデル名を指定:
  #   - my-finetuned-model
  name: llama2
  
  # 学習済みモデルのローカルパス（GGUF変換前、オプション）
  # 指定した場合、convert_to_gguf.pyで変換・登録が必要
  local_path: null

# =============================================================================
# サンプリング設定
# =============================================================================
sampling:
  # 最大生成トークン数
  num_predict: 512
  
  # 温度（0.0で決定論的）
  temperature: 0.7
  
  # Top-p サンプリング
  top_p: 0.9
  
  # Top-k サンプリング
  top_k: 40
  
  # 繰り返しペナルティ
  repeat_penalty: 1.1
  
  # 停止トークン（オプション）
  stop: []
  
  # シード（再現性のため、nullでランダム）
  seed: null

# =============================================================================
# サーバー設定
# =============================================================================
server:
  # Ollama APIホスト
  host: localhost
  
  # Ollama APIポート
  port: 11434
  
  # タイムアウト（秒）
  timeout: 300

# =============================================================================
# プロンプト設定
# =============================================================================
prompt:
  # プロンプトテンプレート
  # 利用可能: "alpaca", "chatml", "llama3", "raw", "none"
  # "none": テンプレートなし（Ollamaのデフォルトを使用）
  template: none
  
  # システムプロンプト
  system_prompt: "You are a helpful AI assistant."
